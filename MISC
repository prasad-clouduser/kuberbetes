
On windows you must rely on virtualization software like Hyper-v or VMWare workstation or virtual box to create linux VMs on which you can run k8s. 
There are other solutions also available to run k8s components as docker containers on windows VMs. Even then the docker images are linux based, and under the hoods they run on a small linux OS created by hyper-v 
for running linux docker containers. 

minikube -> relys on one of the virtualization software like oracle virtual box to create virtual machines that run the k8s cluster components.
kubeadm -> single/multi node cluster setup

Turnkey solution -> VMS should be available prior, then deploy using certified k8s solution (open shift, vagrant, PKS, etc)
hosted -> gke, eks etc. openshift online. 

======================================================================================================

kube api server can be run in active active mode. 
If we have two API server on each master node,  you can have load balancer infront of master nodes that split traffic between API servers. 
kubectl utility point to that load balancer. nginx or ha proxy or any other load balancer for this purpose. 

scheduler and controller should run active and standby mode, otherwise results in more number of pods. 

Controller manager: 
when controller manager process is configured. you may specify the leader elect option which is by default set to true. 
with this option when controller manager process starts it tries to gain or lease a lock on an endpoint object in k8s named as kube controller manager endpoint. 
whichever process first updates the endpoint withits information gains the lease and becomes the active of two. 
it holds the lock for lease duration specified using the leader elect lease duration option. 
the active process renews the lease every 10 sec. 
both theprocesses try to become leader every 2 sec, second process can acquire the lock and become the leader. 
scheduler also same 

if etcd on the control plan node, if control node crashes, etcd also goes down. if it is on separate cluster, it won't go down. 
etcd; if any component want to reach, it can reach the etcd server at any of its instances. 
u can read and write data thorugh any of the available etcd server instances. thats why we specify the list of etcd server in kubeapi server config.

===========================================================================================================================================
if multiple writes comes to write to etcd server. 

only one of the instances is responsbile for processing the writes. 
internally two nodes elects the leader among them.
of the total instances one node becomes the leader. 

if writes came in through any of the follower nodes, then they forward the writes to the leader internally and then the leader processes the writes. 
when writes r processed the leader ensures that copies of writes are distributed to other instances in the cluster. 

if one node is down among 3 nodes, write happesn the leader node, writes is said to be completed only when the write is copied to majority of the nodes. 
Quorum is the minimum number of nodes that must be available for the cluster to function properly or make a successful write. 
recommneded to have a min 3 instances in an etcd cluster. that way it offers falut tolerant of atleast one node. 

Quorum is required to say cluster is alive. 

fault tolerance: the number of nodes that you can afford to lose hwile keeping the cluster alive. 

leader election is done by raft algathrim, it uses timer whicher completes first will be considered as a leader by taking consent from other two. when other two stops reciving 
notifications from leader, they start its own timers to elect the leader. 


========================================================================================================================================================================

virtual box - this is our hypervisor this is ultimately responsible for running our virtual machines. 
vagrant -> automatiion tool it really easy to spin up a whole bunch of VMs. 
==========================================================================================================

both kubelet and container runtime need to interface with these control groups to enfore various resource management for pods. 
so think of things like setting the CPU and Memory requests and limits. These are all things that need to be kind of communicated to the c groups. 

==================================================================================================================================================================

Deployments: 

For example: 
you would like to make multiple changes to your environment. 
such as upgrading underlying webserver versions. 
as well as scalling your environment. 
and also modifying the resoure allocations etc. 
you dont apply the each change immediately after the command is run. instead you would like to apply a pause to the env, and make the changes and then resume so that 
all the changes are rolled out together. 
All of the capbilitis available with k8s deployments. 














