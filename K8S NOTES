ETCD: 
Maintain Information about diff ships
what container is on which ship. 
what time it was loaded etc.

cranes --> kube-scheduler
scheduler identifies the right node to place a container on based on containerr resource requirements, the worker nodes capacity, or any other policies or contraints 
such as taints and tolerations or node afffinity rules that are on them.

operation team take care of ship handling, traffic control etc
they deal with issues related to damages, the route to different ships take etc. 

cargo team takes care of container, container are damaged destroed they make sure that new containers are available. 

service office that takes care of the IT and communications between different ships. 

Node controller --> onboarding new nodes to cluster, handling situations where nodes become unavailable or gets destroyed. 
replication controller -> ensures that desired num of containers are running at all times in a replication group. 

Kube Api server -> orchetrating all operations within the cluster 
expose k8s apis to perform managment operations on the cluster by users. 
as well as various contraollers to monitor the state of the cluster to make necessary changes as required and by the worker nodes to communicate with the server. 

cargo ships. 
captain is responsible for managing all activties on these ships. 
captain is responsbile for liasing with the master ships,
starting with letting the master ship know that they are interested in joinng the group, receiving info about the containers to be loaded on the ship.
loading the appropriate container as required. 
sending reports back to master about the status of the ship 
and status of the containers on the ship etc. 
captain -> kubelet. runs on each node. listen for instructions from the kube API server.
and deploys or destroys containers on the nodes as required. 

kube api server periodically fetches status reports from the kubelets to monitor status of nodes and ocntainer on them
communications between worker nodes are enabled by another component that runs on the worker node known as kube proxy service 
kube proxy service ensures that necessary rules are in place on the worker nodes to allow the containers running on them to reach each other. 

======================================

kubernetis introduced a interface called container runtime interface (CRI)
CRI allowed any vendor to work as container runtime for k8s as long as they adhere to the OCI standards. 
OCI contains image spec and runtime spec. 
an image spec defines the specifications on how an image should be built. 
runtime spac define the standards on how any container runtime should be developed. 

other runtimes continue to use CRI to work with k8s. 
but docker uses dockerrshim to work k8s

docker consists of many components not just runtime. api, cli, build. 
finally also the containerr runtime called runC.
and daemon that manage runc is called containerd. contaierd CRI compatible can work directly with k&s

uncessary effort and added complications becaoz of dockershim, decided in 1.24 to remove dockershim compley support for docker is also removed. . 

=====================
every change that you make to the k8s server such as adding additonal nodes deploying pods or replica sets are updated in etcd server. 
then only change considered to be complete one. 

--advertise-client-url this is address on which edcd listens. 

deploy cluster with kubeadm -> etcd server deployed as pod. 
deploy cluster manually --> etcd run as service on master node. 

initial-advertise url ==> if you have multiple master each want to know each other etcd server deployed on other masters. 
kubeapi configured with etcd server url then only it can interact iwth etcd. 

etcd root directory /registry.
etcd client requires the certs and api version to connect to the etcd server. 

=============================
kubectl command infact reaching the kubeapi server. 

1. kube-apiserver first authenticates the request and validates it. 
2. then it retrieves the data from the etcd cluster and responds back with requested info.
=====================================

1. request is authenticated first and then validated. 
2. api server creates a pod object without assigning it to a node. 
3. updates the info in the etcd server and updates the user tthat pod have been created. 
4. scheduler continuosly monitor the api server and realizes that there is new pod with no node assigned. the scheduler identifies the new node to place the new pod on 
   and communicates that back to the kube-apiserver. then api server updates info in the etcd cluster. 
5. Then api server then passes that info to the kubelet in the appropriate worker node. 
6. kubelet then create the pod on the node and instructs the container runtime engine to deploy the application image. 
7. once done kubelet updates the status back to api server, then api server updates the data back in the etcd cluster. 

kubeapi server run as a pod when deployed the cluster with kubeadm tool. options can be viewed in /etc/kubernetes/manifests/kube-apiserver.yaml
non kubeadm setup inspect the option by viewing the kube-apiserver service located at /etc/systemd/system/kube-apiserver.service. 

===============================================
kube controller manager: 
in the kubernetes terms,  controller is a process that continuosly monitors the state of various components within the system and works towards brining the whole system 
to the desired functioning state. 

node controller -> is responsible for monitoring the status of the nodes and taking necessary actions to keep the applications running. 
it does that through the kube api server. 

1. node controller checks the status of the nodes every 5 sec. 
2. that way the node controller can monitor the health of the nodes .
3. if it stops the receiving the heartbeat from the node the node is marked as unreachabe
4. but it waits for 40 sec before marking it as unreachable. 
5. after marking node as unreachable, it gives it 5 min to come backup if it does not it removes the pod assigned to that node and provision them on healthy ones if 
pods are part of replica set. 

replication controller --> 

responsible for monitor the status of replica sets and ensuring that desired num of pods are available at all times within the set. 
if the pod dies it creates another one. 

all controllers (job controler, node controller, etc etc) --> kind of brain behind lot of things in kubernetes. 
they are all packaged into single process known as the kubernetes controller manager. 

by default all controllers are enabled. --controllers stringSlice. 

=======================================================================================================

scheduler is only responsible for deciding which pod goes on which node. it does not actually place the pod on node. 
kubectl captain of the ship is the one who creates the pod on the ship. 
you need ensure right container on the right ship. 
scheduler decides which nodes the pods are placed on depending on certain crtieria. 
u may have pods with different resource requirements. 
u can have nodes in the cluster dedicated to certain applications. 
two phases: 
consider pod requires 10 cpu 
1. scheduler tries to filter out the nodes that dont fit the profile for this pod. 
2. the scheduler ranks the nodes to identify the best fit for the pod. it uses a pririty function to assign a score to node on a scale of 0 to 10. 
3. calcuates the amount of resources that would be free on the node after placing the pod on them. the more resource free would get the better rank. 

=====================================================================================================================================

Kubelet-> they load or unload containers on the ship as instructed by the scheduler on the master. they also send back the report at the regular intervals on the 
status of the ship and containners on them,

kubelet in the k8s worker node register the node with a kubernetes cluster. when it recieves instructions to load a container or pod on the node, it requests the container runtime engine which may be docker 
to pull the required image and run an instance. 

then kubectl continuosly to monitor the state of the pod and container in it and reports to the kube api server on a timely basis. 
manully install the kubelet on the worker nodes. 

======================================================================================================================================
KUBE-PROXY: 

a pod network is an internal virtual network that spans across all the nodes in the cluster to which all the pods connect to. 
through this network, they are able to communicate with each other. 

how to service gets an ip?? the service can't join the pod network, service is not an actual thing, it is not a container like pods, so it does not have any interfaces or an actively listening process. 
it is virtual component that only lives in the kubernetes memory. 
but then we also said that the service should be accessible across the cluster from any nodes. how this is achieved? that is where kube-proxy comes in.
kubeproxy is the process that runs on each node in the k8s clsuter. its job is to look for new services and everytime a service is created, it creates the appropriate 
rules on each node to forward traffic to those services to the backend pods. one way it does it using iptable rules .
it creates the iptable rules on each node like (Service IP mapping to POD IP)

====================

two containers (Helper and actual app container) can also communicate with each other directly by referring to each other as local host since they share the same network space. 
plus they can easily share the same storage space as well. 

labels is a dictionary within the metadata dictionary. 

=================================

replication controller helps us to run multiple instances of a single pod in the kubernetes cluster. thus proviing HA. 
balance the load across multiple pods on mulitple nodes. 

selector is differece between replication controller (old tech but still selector field is there) and replica set.


