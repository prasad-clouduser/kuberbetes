ETCD: 
Maintain Information about diff ships
what container is on which ship. 
what time it was loaded etc.

cranes --> kube-scheduler
scheduler identifies the right node to place a container on based on containerr resource requirements, the worker nodes capacity, or any other policies or contraints 
such as taints and tolerations or node afffinity rules that are on them.

operation team take care of ship handling, traffic control etc
they deal with issues related to damages, the route to different ships take etc. 

cargo team takes care of container, container are damaged destroed they make sure that new containers are available. 

service office that takes care of the IT and communications between different ships. 

Node controller --> onboarding new nodes to cluster, handling situations where nodes become unavailable or gets destroyed. 
replication controller -> ensures that desired num of containers are running at all times in a replication group. 

Kube Api server -> orchetrating all operations within the cluster 
expose k8s apis to perform managment operations on the cluster by users. 
as well as various contraollers to monitor the state of the cluster to make necessary changes as required and by the worker nodes to communicate with the server. 

cargo ships. 
captain is responsible for managing all activties on these ships. 
captain is responsbile for liasing with the master ships,
starting with letting the master ship know that they are interested in joinng the group, receiving info about the containers to be loaded on the ship.
loading the appropriate container as required. 
sending reports back to master about the status of the ship 
and status of the containers on the ship etc. 
captain -> kubelet. runs on each node. listen for instructions from the kube API server.
and deploys or destroys containers on the nodes as required. 

kube api server periodically fetches status reports from the kubelets to monitor status of nodes and ocntainer on them
communications between worker nodes are enabled by another component that runs on the worker node known as kube proxy service 
kube proxy service ensures that necessary rules are in place on the worker nodes to allow the containers running on them to reach each other. 

======================================

kubernetis introduced a interface called container runtime interface (CRI)
CRI allowed any vendor to work as container runtime for k8s as long as they adhere to the OCI standards. 
OCI contains image spec and runtime spec. 
an image spec defines the specifications on how an image should be built. 
runtime spac define the standards on how any container runtime should be developed. 

other runtimes continue to use CRI to work with k8s. 
but docker uses dockerrshim to work k8s

docker consists of many components not just runtime. api, cli, build. 
finally also the containerr runtime called runC.
and daemon that manage runc is called containerd. contaierd CRI compatible can work directly with k&s

uncessary effort and added complications becaoz of dockershim, decided in 1.24 to remove dockershim compley support for docker is also removed. . 

=====================
every change that you make to the k8s server such as adding additonal nodes deploying pods or replica sets are updated in etcd server. 
then only change considered to be complete one. 

--advertise-client-url this is address on which edcd listens. 

deploy cluster with kubeadm -> etcd server deployed as pod. 
deploy cluster manually --> etcd run as service on master node. 

initial-advertise url ==> if you have multiple master each want to know each other etcd server deployed on other masters. 
kubeapi configured with etcd server url then only it can interact iwth etcd. 

etcd root directory /registry.
etcd client requires the certs and api version to connect to the etcd server. 

=============================
kubectl command infact reaching the kubeapi server. 

1. kube-apiserver first authenticates the request and validates it. 
2. then it retrieves the data from the etcd cluster and responds back with requested info.
=====================================

1. request is authenticated first and then validated. 
2. api server creates a pod object without assigning it to a node. 
3. updates the info in the etcd server and updates the user tthat pod have been created. 
4. scheduler continuosly monitor the api server and realizes that there is new pod with no node assigned. the scheduler identifies the new node to place the new pod on 
   and communicates that back to the kube-apiserver. then api server updates info in the etcd cluster. 
5. Then api server then passes that info to the kubelet in the appropriate worker node. 
6. kubelet then create the pod on the node and instructs the container runtime engine to deploy the application image. 
7. once done kubelet updates the status back to api server, then api server updates the data back in the etcd cluster. 

kubeapi server run as a pod when deployed the cluster with kubeadm tool. options can be viewed in /etc/kubernetes/manifests/kube-apiserver.yaml
non kubeadm setup inspect the option by viewing the kube-apiserver service located at /etc/systemd/system/kube-apiserver.service. 


