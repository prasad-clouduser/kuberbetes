ETCD: 
Maintain Information about diff ships
what container is on which ship. 
what time it was loaded etc.

cranes --> kube-scheduler
scheduler identifies the right node to place a container on based on containerr resource requirements, the worker nodes capacity, or any other policies or contraints 
such as taints and tolerations or node afffinity rules that are on them.

operation team take care of ship handling, traffic control etc
they deal with issues related to damages, the route to different ships take etc. 

cargo team takes care of container, container are damaged destroed they make sure that new containers are available. 

service office that takes care of the IT and communications between different ships. 

Node controller --> onboarding new nodes to cluster, handling situations where nodes become unavailable or gets destroyed. 
replication controller -> ensures that desired num of containers are running at all times in a replication group. 

Kube Api server -> orchetrating all operations within the cluster 
expose k8s apis to perform managment operations on the cluster by users. 
as well as various contraollers to monitor the state of the cluster to make necessary changes as required and by the worker nodes to communicate with the server. 

cargo ships. 
captain is responsible for managing all activties on these ships. 
captain is responsbile for liasing with the master ships,
starting with letting the master ship know that they are interested in joinng the group, receiving info about the containers to be loaded on the ship.
loading the appropriate container as required. 
sending reports back to master about the status of the ship 
and status of the containers on the ship etc. 
captain -> kubelet. runs on each node. listen for instructions from the kube API server.
and deploys or destroys containers on the nodes as required. 

kube api server periodically fetches status reports from the kubelets to monitor status of nodes and ocntainer on them
communications between worker nodes are enabled by another component that runs on the worker node known as kube proxy service 
kube proxy service ensures that necessary rules are in place on the worker nodes to allow the containers running on them to reach each other. 

======================================

kubernetis introduced a interface called container runtime interface (CRI)
CRI allowed any vendor to work as container runtime for k8s as long as they adhere to the OCI standards. 
OCI contains image spec and runtime spec. 
an image spec defines the specifications on how an image should be built. 
runtime spac define the standards on how any container runtime should be developed. 

other runtimes continue to use CRI to work with k8s. 
but docker uses dockerrshim to work k8s

docker consists of many components not just runtime. api, cli, build. 
finally also the containerr runtime called runC.
and daemon that manage runc is called containerd. contaierd CRI compatible can work directly with k&s

uncessary effort and added complications becaoz of dockershim, decided in 1.24 to remove dockershim compley support for docker is also removed. . 

=====================
every change that you make to the k8s server such as adding additonal nodes deploying pods or replica sets are updated in etcd server. 
then only change considered to be complete one. 

--advertise-client-url this is address on which edcd listens. 

deploy cluster with kubeadm -> etcd server deployed as pod. 
deploy cluster manually --> etcd run as service on master node. 

initial-advertise url ==> if you have multiple master each want to know each other etcd server deployed on other masters. 
kubeapi configured with etcd server url then only it can interact iwth etcd. 

etcd root directory /registry.
etcd client requires the certs and api version to connect to the etcd server. 

=============================
kubectl command infact reaching the kubeapi server. 

1. kube-apiserver first authenticates the request and validates it. 
2. then it retrieves the data from the etcd cluster and responds back with requested info.
=====================================

1. request is authenticated first and then validated. 
2. api server creates a pod object without assigning it to a node. 
3. updates the info in the etcd server and updates the user tthat pod have been created. 
4. scheduler continuosly monitor the api server and realizes that there is new pod with no node assigned. the scheduler identifies the new node to place the new pod on 
   and communicates that back to the kube-apiserver. then api server updates info in the etcd cluster. 
5. Then api server then passes that info to the kubelet in the appropriate worker node. 
6. kubelet then create the pod on the node and instructs the container runtime engine to deploy the application image. 
7. once done kubelet updates the status back to api server, then api server updates the data back in the etcd cluster. 

kubeapi server run as a pod when deployed the cluster with kubeadm tool. options can be viewed in /etc/kubernetes/manifests/kube-apiserver.yaml
non kubeadm setup inspect the option by viewing the kube-apiserver service located at /etc/systemd/system/kube-apiserver.service. 

===============================================
kube controller manager: 
in the kubernetes terms,  controller is a process that continuosly monitors the state of various components within the system and works towards brining the whole system 
to the desired functioning state. 

node controller -> is responsible for monitoring the status of the nodes and taking necessary actions to keep the applications running. 
it does that through the kube api server. 

1. node controller checks the status of the nodes every 5 sec. 
2. that way the node controller can monitor the health of the nodes .
3. if it stops the receiving the heartbeat from the node the node is marked as unreachabe
4. but it waits for 40 sec before marking it as unreachable. 
5. after marking node as unreachable, it gives it 5 min to come backup if it does not it removes the pod assigned to that node and provision them on healthy ones if 
pods are part of replica set. 

replication controller --> 

responsible for monitor the status of replica sets and ensuring that desired num of pods are available at all times within the set. 
if the pod dies it creates another one. 

all controllers (job controler, node controller, etc etc) --> kind of brain behind lot of things in kubernetes. 
they are all packaged into single process known as the kubernetes controller manager. 

by default all controllers are enabled. --controllers stringSlice. 

=======================================================================================================

scheduler is only responsible for deciding which pod goes on which node. it does not actually place the pod on node. 
kubectl captain of the ship is the one who creates the pod on the ship. 
you need ensure right container on the right ship. 
scheduler decides which nodes the pods are placed on depending on certain crtieria. 
u may have pods with different resource requirements. 
u can have nodes in the cluster dedicated to certain applications. 
two phases: 
consider pod requires 10 cpu 
1. scheduler tries to filter out the nodes that dont fit the profile for this pod. 
2. the scheduler ranks the nodes to identify the best fit for the pod. it uses a pririty function to assign a score to node on a scale of 0 to 10. 
3. calcuates the amount of resources that would be free on the node after placing the pod on them. the more resource free would get the better rank. 

=====================================================================================================================================

Kubelet-> they load or unload containers on the ship as instructed by the scheduler on the master. they also send back the report at the regular intervals on the 
status of the ship and containners on them,

kubelet in the k8s worker node register the node with a kubernetes cluster. when it recieves instructions to load a container or pod on the node, it requests the container runtime engine which may be docker 
to pull the required image and run an instance. 

then kubectl continuosly to monitor the state of the pod and container in it and reports to the kube api server on a timely basis. 
manully install the kubelet on the worker nodes. 

======================================================================================================================================
KUBE-PROXY: 

a pod network is an internal virtual network that spans across all the nodes in the cluster to which all the pods connect to. 
through this network, they are able to communicate with each other. 

how to service gets an ip?? the service can't join the pod network, service is not an actual thing, it is not a container like pods, so it does not have any interfaces or an actively listening process. 
it is virtual component that only lives in the kubernetes memory. 
but then we also said that the service should be accessible across the cluster from any nodes. how this is achieved? that is where kube-proxy comes in.
kubeproxy is the process that runs on each node in the k8s clsuter. its job is to look for new services and everytime a service is created, it creates the appropriate 
rules on each node to forward traffic to those services to the backend pods. one way it does it using iptable rules .
it creates the iptable rules on each node like (Service IP mapping to POD IP)

========================================================================================

two containers (Helper and actual app container) can also communicate with each other directly by referring to each other as local host since they share the same network space. 
plus they can easily share the same storage space as well. 

labels is a dictionary within the metadata dictionary. 

=======================================================================================

replication controller helps us to run multiple instances of a single pod in the kubernetes cluster. thus proviing HA. 
balance the load across multiple pods on mulitple nodes. 

selector is differece between replication controller (old tech but still selector field is there) and replica set.
The deployment automatically creates a ReplicaSet, replicaset ultimately creates the pods. 
deployment uses for usecases, rolling updates, pause upgrade and start. 
=============================================================================================
Services: 

Service listens to a port on the node and forwards requests to the pods. 
The port on the pod where the actual web server is running is 80 and referred to as targetport. this is where service forwards the requests to. 
second port--> port on the service itself, it is simply referred to as port. 
Service is infact like a virtual server inside the node. inside the cluster it has its own IP address. this is called cluster IP of the service. 
finally we have port on the node itself. which we use to access the web server externally. that is known as node port. 

target port optional if not provided will be same as Port field. 
nodePort if not provided, will be taken from the free port 
The service automatically select the all three pods as end points to forwards the external requests coming from user. 
it uses random algarithm to balance the load. thus service acts built in load baalncer to distribute the load across different pods. 

k8s automatically creates a service that spans across all the nodes in the cluster and maps the targetPort to the same nodePort on all the nodes in the cluster. 
we can use any node in the cluster to launch 
when pods are removed or added, the service is automatically updated, making it highly flexible and adaptive.

Each service gets an IP and name assigned to it inside the cluster, and that is the name that should be used by other pods to access the service, this type of service 
is known as Cluster IP. 

Nodeport scenario: 
Even if your pods are only hosted on two of the nodes, they will still be accessible on the IPs of all the nodes in the cluster. 

================================================================================================================================================

LOAD BALANCER: 

Load balancer scenario: how to give single URL instead of multiple node and nodeport URL like nodeport service?

Oneway to achieve this is, to create a new VM for load balancer purpose. 
install and configure suitable load balancer on it like HA proxy/nginx 
then configure the load balancer to route traffic to underlying nodes. 

k8s has support for integrating with the native load balancers of certain cloud providers in configuring this LB for us. 
so all you need to do it set the service type as LoadBalancer. 

============================

namespaces: 

for internal purpose, such as networking solution, the DNS service, k8s creates these under kube-system namespace. (created at cluster startup) 
resources that should be made available to all users are created --> kube-public. 
dev, prod, default namespaces, each of these namespaces can have its own set of policies to define who can do what.
u can also assign quota of resources to each of these namespaces. this way each namespace is guranteed a certain amount and does not use more than its allowed limit. 

within namepsace, resource can refer to each other with just first name, no need to use full name. 
if required webapp pod can reach service in another namespace as well.
for this you must append the name of the namepsace to the name of the service.
when a service is created a DNS entry is added automatically in this format. (db-service.dev.svc.cluster.local)

cluster.local -> default domain name of the k8s cluster.
svc -> sub domain for the service. 
dev --> namespace. 
db-service -> service name. 

to limit resources in a namespace, create a resource quota. 

============================================================================================================

Imperatvie & Declarative

imperative appraoch -> step by step approach. 

declartive approach -> we just specify the final destination, we don't tell step by step instructions. declarting final destination. 
system figures out the right path to reach the destination. Specifying what to do, not how to do. 

Imperative approach; these commands run once and forgotten. 
they are only available in session history of the user who ran these commands.
so its hard for another person to figure out how these objects were created. 

declarative apporach -> exacctly what we need to have, need to define in the yaml format. 
create, replace, commands with kubectl --> imperative apporach.  object should already exist if we use these commands otherwise error will be thrown. 

when you use edit command it changes are applied to objects, but that does not tracked in source code repository. 
kubectl edit -> modifying the live object, changes are not recorded anywhere. this will open up a manifest file in the kubernetes memory. 

Declartive: 
we use apply command instead of create, replace. 
apply command intelligent enough to create the object if it doesnot already exist. 
complext scenario -> multi container pod -> object config created and use apply command. 


===================================================================================================

APPLY: 

When you create an object with apply command, it does bit more, first traslates the yaml to json format 
And then it is stored as the last applied configuration. 
Object also created and live object configuration is also created. 

Going forward for any update to the object, all 3 (Localfile, Last applied config, Live object config) are compared to identify what changes are to be made on the live object. 

kubectl create or replace command don't store the last applied config like this. (as annotations)
last applied confi is used to check if the fields can be deleted. 







