
If the node comeback online immediately, the kubelet process starts and pods comeback online. 
however if the node node was down more than 5 min, pods are terminated from that node. 
k8s considers them as DEAD. 
if the pods were part of replica set, then they are recreted on other nodes. 
The time it waits for a pod to comeback online is known as POD eviction timeout and is set on the controller manager with a default value of 5 min. 

whenver node goes offline, the master node waits for 5 min before considering node as Dead. 

if you know world load running on the node have other replicas. 
if its okay that they go down for a short period of time.
and if you are sure the node will come back within five minute. 
you can make a quick upgrade and reboot. 

other methods: 

drain: 
when you drain a node, the pods are grcefully terminated on the node from the node that they are on and recreated on another node. 
the node is marked as cordoned or marked as unschedulable. no pods are scheduled on this node until you remove the restriction. 
you then need to uncordon it so that pods can be scheduled on it again. (only new or deleted pods are rescheduled onthis, not moved pods before) 

or you can simply cordon a node -> mark as unschedulable, does not happen anythign to exsting to pods, new pods are not scheduled on this node. 


===================================================================================================================================================================
k8s versoins: 

All bug fixes and improvments first go into Alpha release. tagged as alpha. In this release the features are disabled by default and may be buggy. 
then from there, they make their beta release where the code is well tested. the new features are enabled by default. 
and finally they make their way to the main stable release. 

==========================================================================================================================
k8s api server is the main component, none of the other component should be at a version higher than the kube api server. 

kube api server at 1.10
kube controller manager & kube schedule at one version lower. 
kubelet & kube-aproxy could be two version lower to api server. 
kubectl could be one verson higher and one version lower. 

k8s supports only two lower version only to current version. 
ex: if you are at 1.13, it will support till 1.11

if you are upgrading k8s, suppose upgrading master nodes, all components on it briefly go down. 
can't interact with kubectl, workloads on nodes are not impacted. you can create new workloads also. 
if pods were to fail, a new pod wont be created automatically. 
as long as nodes and pods are up, app is up, your users won't be impacted. 

3 strategies: 

upgrade all nodes at a time. down time for application is expected. 
upgrade one node a time and share the pods to other nodes. 
nodes with newer software version can be added to the cluster, moe workload over to the new and decomission old node. 

kubectl get nodes give the output which display the version of kubelets on each of these nodes registered with API servers.
upgrade them on masernode and nodes and check again. 
upgrade kubeadm and kubelet on the master node and nodes and restart the kubelet service, on nodes first you need to drain the node, reschedule pod on another nodes and start uprading kubeadm and kubelet. 


===================================================================================================================================
On master node: 

upgrade the kubeadm
run the kubeadm upgrade plan 
check the instruction and upgrade it. 

for node upgrade. 

upgrade the node using kubeadm
later drain the node. (from control plan) 
upgrade kubelet process on it. 
restart the kubelet process. 
uncordon the node. 

follow the same for other nodes. 

====================================================================================================================================
backup & restore: 

etcd stores info about state of our cluster. info about our cluster itself, the nodes and every other resources created within the cluster, are stored here. 
when etcd restores from a backup, it initializes a new cluster configuration and configures the members of etcd as new members to a new cluster. 
this is to prevent new member from accidentally joining an existing cluster. 










