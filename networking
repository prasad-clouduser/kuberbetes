
We have two system A and B. 
how does System A reach System B. 
We connect them to a switch. 
Switch creates a network containing the two systems. 

To connect them to a switch we need an interface on each host. 
to see the interfaces on host, we use ip link command. 
we use here eth0 to connect to the switch. 

once ip address assigned to system, computers can communicate with each other through the switch. 
switch can only enable communication within network. 
which means it can receive packets from a host on the network and deliver it to the other systems within same network. 

A router helps connect two networks together. 
router - think of it as another server with many network ports. 
since it connects to the two separate networks, it gets two IPs assigned. router gets two IPs. in the first network and second network. 

To confgure a gateway on system B (which is in network 1.0)  to reach the systems on network 2.0, use the ip route add command and specificy that you can reach the 
2.0 network via thorough the door or gateway at 192.168.1.1 

Suppose system need access to the internet. 

1. You connect the router to the internet. (ip route add 172.217.194.0/24 via 192.168.2.1) 
2. Then add a new route in yur routing table to route all traffic to the network 172.217.194 through your router. 

ip link is to list and modify the interfaces on the host. 
ip addr is to list ip addresses assigned to those interfaces. 
ip addr add is to set the ip addresses on the interfaces. 
================================================================================================================================

ip route add default via 192.168.2.1
This way any request to any network outside of your existing network goes to this particular router. (instead of creatign multiple routes for various network in route table)

All you need is a single routing table entry with the default gateway set to the router's IP address.
instead of default you could also say 0.0.0.0 (any IP destination) 

0.0.0.0 field in the gateway field indicatees that you don't need a gateway, for ex; to access any device in the 192.168.2.0 network it does not need a gateway because it is
in its own network. 

two routers present? 

1. one router should be used to connect to the public networks. 
2. one router is used for connecting to the internal private network. 
Total two routes present. 

how to setup a linux host as a router? 

In linux packets are not forwarded from one interface to the next.
for example packets received on eth0 on host B are not forwared to elsewhere through eth1 
this is this way for security reason for ex: if you had eth0 connected to your private network and eth1 to a public network, we don't want anyone from public network 
to easily send msg to private network. 

ip addr command is to see the IP address assigned to those interfaces. 
ip addr add command is to set IP addresses on the interfaces. 

===========================================================================================

How system know where the router is there on the network to send the packets through?
The router is just another device on the network, there could be many other such devices. 
thats where we configure the systems with a gateway or route. 
if the network was a room, the gateway is a door to the outside world to the other networks or to the internet. 
the systeems need to know where the door is to go through that. run the route cmd to see that. 

================================================================================

DNS: 

Decided to move all the entries in /etc/hosts to single server who will manage it centrally. 
we call that as our DNS server. 

how do we point our host to DNS server?
every host has DNS resolution configuration file at /etc/resolv.conf 
you add an entry init specifying address of the DNS server. 

top level domain --> they represent the intent of the website. --> .com 

.com queries google and then google quieries www. 

=============================================================
NETWORK Namspaces: 

Network namespaces are used by containers like docker to implement network isolation. 
contaners are separated from underlying host using namespaces. 

we create a separate namespace for the container (for isoliation to not see any other container) 
container see only processes run by it, thinks that its on its own host. 
underlying host have visibility into all of the processes including >processes run by the container. 

our host has its own routing and ARP tables with info about the rest of the network. 
we want to seal all of those details from the container. when container is created, we create a network namespace for it, that way it has no visibility to any 
network related information on the host. 
within namespace container has its own virutal interfaces, routing and ARP tables. 

to view the interface on namespace just executte the ip netns exec red ip link ( you can understtand successfuly prevented containerr to see host interfaces with the namespaces) 

how do enable all namespaces to communicate with each other?
just like the physcal world, yu create a virtual network inside the host. 
to create a network you need a switch, to create virtual network you need a virtual switch
mutliple solutions available for this. native solutions (linux bridge ) and open V switch etc. 
We crate a interface on the host for this, which is just like other interfaces. (run ip link to see) 
think of it as interface for the host and a switch for the namespaces.
next step is to connect namespaces to this new virtual network switch. 
create two interfaces, attach one interface to red namespace (with netns command) and then other peer interface (connect to bridge network) to switch and made this as master. 
similary do the same thing for other two namespaces. 

How do we establish communication betweeen my host and these namespaces?
remember bridge switch is actually a just another network interface on the host, just assign an ip address to this interface (which acts like a switch to namespaces)

How to provide a gateway or door to the outside world from the namespace? Means how to ping other hosts (connected with LAN) from this host namespaces?

A door or gateway is a system on the local network that connects to the other network
so what is a system that has one interface on the network local to the blue namespace which 192.168.15 network and is also connected to the outside LAN network.  
our local host is the gateway that connects the two networks together ( namespaces network 192.168.15.0, we assigned IP address to v-net-0 interface which is like switch for 
namespaces which can ping each other) 
we can add a route entry in the blue namespace to say route all traffic to the 192.168.1 network through the gateway at 192.168.15.5 (v-net-0 ip address) 
because blue namespace can only reach the gateway in its local network at 192.168.15.5, we can't use other interface ip (192.168.1.2) 

user try to reach other Host, just NAT the IPs 192.168.15.5 (so that these will be replaced with its own ip addresses) so that other systems can respond back.
for this we need NAT enable on our host acting as gateway here so that it can send the msgs to the LAN in its own name with its own address.
MASCURADE all packets coming from the source network withits own IP addresse) 


=======================================================================================================================

Docker Networking: 

1. with the host network, the container is attached to the host network. There is no network isolation between the host and container.
2. if you run web app that runs on port 80 on container, without using port mapping it runs the same port 80 on the host. 
3. if you run same container again, it won't run as they use same host networking. 

bridge: 
an internal private network is created which the docker host and container attach to. 
network has address 172.17.0.0 by default and each device connecting to this network get their own internal private network address on this network. 4

after installing docker it creates network called bridge. 
but it display as docker0 in the command ip link 
port mapping creates the NAT rules for port mapping. to forward traffic from 8080 to container port 80
here also two pairs of network interfaces are created
docker0 is like switch and other interface on network namespace.

======================================================================================================================================

CNI: 

bridge program you create to attach the namespace to the network -> automated script.

suppose you want to create it for other networking type automated script
1. how do you make sure the program (like bridge) yu create will work correctly with these runtimes?
2. how do we know that container runtimes like k8s or rocket will invoke your program correctly?
3. thats where we need some standards defined. 
4. A standard that define how a program should look. How container runtime should invoke them so that everyone can adhere to a single set of standards. 
and develp solution that work across runtimes. thats where container network interface comes in. 

CNI is set of standards that define how a program should be developed to solve networking challenges in a contaienrr runtime environments. 

CNI defines some responsibilites to Containers Runtimes and Plugins. (check the list) 
As long as container runtimes and plugins adhere to these standards. they can all live together in harmony. 

Docker does not support CNI, however we can use these all plugins with some workaround. 
it supports CNM. another standard. 

How doccker creates?
1. when k8s creates docker containers, it creates them on NONE network. 
2. it then invokes the configured CNI Plugins who takes care of the rest of the configuration. 

==================================================================================

Cluster Networking: 

networking confi required on the master and worker nodes in k8s cluster. 
check the ports for the control plan components. 

==========================================================================================

Talked about network that connects nodes together. 
there is also the another layer of networking that crucial for cluster functinaling that networking at pod layer. 

k8s laid out requirements for pod networking. 
we know how to create cable and attach one interface to namespace and another to bridge interface. 

The kubelet on each node is responsible for creating containers whenever a container is created. 
the kubelet looks at CNI configuration passed as command line argument when it was run and identifies our scripts name
it then looks in the CNI's bin directory to find our script and then executes the script with the add command and name and namespace id of the container. 

===============================================================================================

CNI in k8s: 

1. where do we specify the CNI plugins for k8s to use? 
The CNI plugin must be invoked by the component within k8s that is responsible for creating containers because that component must then invoke the appropriate network plugin 
after the container is created. 

The CNI bin directory has all the supported CNI plugins as executables. such as bridge, dhcp, flannel etc.
config directory -> where kubelet looks to find out which plugin needs to be used. 
in this case it finds the bridge confi file. if multiple files are there, it will choose the one in alphabtical order. 

=================================================================================================

CNI Weave: 

when a packet is sent from one pod to another pod , it goes out to the network, to the router and finds its way to the node that hosts the pod. 
This will be a complex process if we have many nodes and many pods in a big cluster. routing table not enough (office can't keep track all routes) 

where weave CNI plugin is deployed on a cluster, it deploys an agent or service on each node. 
tbey communicate with each other to exchange info regarding the nodes and networks and pods within them. 
each peer or agent stores topology of entire setup that way they know the pods and their IPs on the other nodes. 
weave creates its own bridge on the nodes and names it WEAVE then assign IP address to each network. 

single pod can attach multiple bridge networks. 
you could have pod attached to the weave bridge as well as the docker bridge created by docker
what path a packet takes to reach destination depends on the route configured on the container. 

weave make sure that pods gets correct route configured to reach the agent. 
and the agent then takes care of other pods. 

When a packet is sent from one pod to another on another node. 
weave intercept the packet and identifies that its on a separate network. 
it then escapsulates this packet into a new one with new source and destination and sends it across the network. 

once on the other side the other weave agent retrieve the packet and decasulates it and routes the packet to right pod. 

===============================================================================================================================================

IP Address Management - Weave: 

CNI says it is the responsibility of the CNI plugin the network solution provider to take care of assigning IPs to the containers. 
previously we used a script (basic plugin ) we built earlier which took care of assigning IP addresses within this plugin.
How do we manage this ips to make sure duplicate IPs are not assigned to pods. 
instead of coding by ourslf in script, CNI comes with two built in plugin-ins to which you can outsource this task to. 
in our case the plugin in that implemented the approach that we followed for managing the IP address locally on each host is the host local plugin. 

Weave gives range 10.32.0.0/12 for entire network.
which is milion ips. 
from this range, the peers decides to split the IP addresses equally between them and assigns one portion to each node. 
Pods created on these nodes wil have IPs in this range. 

=========================================================================================================================================

Service Networking: 

in previous lectures we talked about pod networking, how  bridge network are created within each node and how a pod get a namespace created for them. and how interfaces
are attached to those namespaces. and how pods gets an ip address assigned to them within the subnet assigned for that node. 

if you want to access services hosted on another pod, you would always use a service. 

when a service is created, it is accessible from all pods on the cluster irrespective of what nodes the pods are on. 
while pod is hosted on a node, a service is hosted across the cluster. it is not bound to specific node. 

kubelet creates PODs on the nodes, it then invokes the CNI Plugin to configure networking for that pod. 
similarly each nodes run another component kube-proxy, which watches the changes in the cluster through kube api server. 
and everytime a new service is to be created, kube-proxy gets into action. 

service is just an virtual object, but how do these get IP addresses?
when service object created, it is assigned an Ip address from a predefined range. 
The kube-proxy components running on each node gets that IP address and creates forwarding rules on each node in the cluster. any traffic coming to this IP of the service, 
should go to the IP of the POD. 
whenever services are created or deleted, the kube-proxy component creates or deletes these rules 

how rules are created: 
3 ways; 
userspace: where kube-proxy listens on a port for each service and proxy's connections to the pods 
by creating IPVS rules 
ip tables.

services ip address range specified in the kube api server's option called --service-cluster-ip-range. (default 10.0.0.0/24)
when setup pod networking, I provided a pod network CIDR range of 10.244.0.0/16
these should not overlap.

=====================================================================================================================================

Cluster DNS: 

K8s deploys a built-in DNS server by default when you setup a cluster.
if you setup k8s manually then you do it by yourself. 

whenever a service is created, the kubernetes DNS service creates a record for the service. 
it maps the service name to the Ip address. 

if test pod, web pod, web pod service are in the same namespace, from test pod we can simply refer as http://web-service 
but if test pod in one namespace and web pod and web pod service in another namespace apps, to refer service from test pod, you would have to say 
http://web-service.apps
for each namespace the DNS server creates Sub domain. 
All services are grouped to gether into another sub domain called SVC. 

All pods and services in a namespace are thus grouped together within a subdomain in the name of the namespace.
All services are further grouped together into another sub domain called SVC. 
so you can reach your application with the name webservice.apps.svc 
finally all pods and service are grouped together into a root domain for the cluster which is set to cluster.local by default.



================================

POD DNS: 
instead of adding the entries into each other /etc/hosts file. 
we move the pod host and ip into a central DNS server 
we then point these pods to the DNS server by adding an entry into their /etc/resolv.conf, specifying that the nameserver is at the ip address of the DNS server. 

with k8s 1.12 the recommneded DNS server is coreDNS. 
kubernetes deploys two pods for coreDNS in replica set in deployment.
it reads a conf file Corefile. 
in this file, the plugin that makes coreDNS work with kubernetes is the kubernetes plugin and that is where the top level domain of the cluster is set, cluster.local
so every record in the coreDNS server falls under this domain. 

any record that DNS server can't solve, for if pod tries to reach out to www.google.com
it is forwarded to the nameserver specified in the coreDNS pods /etc/resolv.conf  
the /etc/resolv.conf file is set to use the nameserver from the k8s node.

coreDNS pod have the service named kube-dns by default. 
ip address of this service is configured as name server on pods. 
on this pod /etc/resolv.conf also have the search entry 

==============================================================================

ingress: 

You bring in another layer proxy server that proxies the rquests on 80 to 38080 on your nodes. 
Point the DNS to this proxy server, user can visit the site by http://my-online-store.com

we need a service to expose the igress controller to the xternal world. selector shuld point to ngix controller. 
You could get different domain names to reach ur cluster by adding multiple DNS entries all pointing to same ingress controller service on kubernetes cluster.

as mentioned before, ingress controller have additinal intelligence to built into them to monitor the k8s cluster for ingress resources and configure the underlying 
nginx server when something is changed. but for ingress controller to do this it requires a service account with right set of permissions. 

you must also passin two env variables that carries the pod name and pod namespace it is deployed to.
nginx service requires these to read the configuration data from within the POD. 









